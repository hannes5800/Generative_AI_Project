{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b8d7a51",
   "metadata": {},
   "source": [
    "# Project Plan – LLM-Powered Crypto Whitepaper & Tokenomics Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5987d9d1",
   "metadata": {},
   "source": [
    "## 1. Project Idea \n",
    "\n",
    "We build an **LLM-based assistant** that helps users understand and compare **crypto projects and their tokenomics** by reading their **whitepapers and official documentation**.\n",
    "\n",
    "The assistant will:\n",
    "- Answer questions like *“How does staking work in Project X?”*, *“Compare the tokenomics of Project A and B”*, or *“What are the main risks mentioned in the docs?”*.\n",
    "- Use **retrieval-augmented generation (RAG)** to ground answers in the actual documents and **cite sources**.\n",
    "- Optionally **generate explanatory images/infographics** (e.g., token distribution diagrams) via a function-calling interface to an AI image generator.\n",
    "\n",
    "**User interface (concept):**\n",
    "- The user interacts with the system through a **conversational interface** in the notebook (text prompts).\n",
    "- The system returns:\n",
    "  - A **text answer** (structured explanation, comparisons, pros/cons, risks).\n",
    "  - **References to specific document passages** used as evidence.\n",
    "  - Optionally, a **generated image** that visualizes tokenomics or concepts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add92917",
   "metadata": {},
   "source": [
    "## 2. Mapping to Course Requirements\n",
    "\n",
    "We implement the following components:\n",
    "\n",
    "1. **Retrieval Augmentation / RAG**\n",
    "   - We create a **small document corpus** (e.g., 3–5 whitepapers / official docs of selected projects).\n",
    "   - We build an **index** (e.g., using text chunks + embeddings).\n",
    "   - For each user query, we:\n",
    "     - Retrieve relevant document passages.\n",
    "     - Pass them as context to the LLM.\n",
    "     - Ask the LLM to **cite which passages** support the answer.\n",
    "\n",
    "2. **Multi-step LLM Pipeline**\n",
    "   - We design a pipeline with **multiple coordinated LLM calls**, for example:\n",
    "     1. **Question analysis / router:**  \n",
    "        Detect the project(s) and type of question (overview, tokenomics, risk, comparison).\n",
    "     2. **Retrieval:**  \n",
    "        Use the router’s decision to form search queries and retrieve chunks from the index.\n",
    "     3. **Answer generation:**  \n",
    "        Generate an answer that:\n",
    "        - Uses the retrieved context,\n",
    "        - Has a clear structure (e.g., overview / details / risks),\n",
    "        - Includes citations.\n",
    "     4. **Answer review (optional second LLM call):**  \n",
    "        Check that:\n",
    "        - The answer is grounded in the retrieved text,\n",
    "        - Citations are present,\n",
    "        - Style guidelines are followed (e.g., disclaimers, no financial advice).\n",
    "\n",
    "3. **Function-calling Interface to an AI Image Generator**\n",
    "   - We design a **tool/function** (e.g., `generate_tokenomics_image(prompt: str)`).\n",
    "   - The LLM can decide to call this tool when:\n",
    "     - The user explicitly asks for a visualization, or\n",
    "     - The question is about tokenomics/flows where a diagram is helpful.\n",
    "   - The tool calls an **external image generation API** (e.g., DALL·E / Stable Diffusion-like service).\n",
    "   - The result (e.g., an image URL or path) is returned to the user.\n",
    "   - The function-calling interface is implemented as part of the LLM tool set.\n",
    "\n",
    "**Optional goals (if time allows):**\n",
    "- Simple **evaluation** comparing:\n",
    "  - Plain LLM (no documents) vs.\n",
    "  - RAG-based system (with documents).\n",
    "- Additional features like:\n",
    "  - Query rewriting or re-ranking in RAG.\n",
    "  - A tiny **synthetic fine-tuning** (e.g., style adaptation) on crypto Q&A.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af709d5",
   "metadata": {},
   "source": [
    "## 3. System Architecture (High-Level)\n",
    "\n",
    "### 3.1 Data / Knowledge Base\n",
    "\n",
    "- **Corpus:**  \n",
    "  A curated set of ~3–5 crypto projects. For each project, we collect:\n",
    "  - Whitepaper or litepaper (converted to plain text).\n",
    "  - Optionally short official documentation pages (e.g., tokenomics sections).\n",
    "- **Preprocessing:**\n",
    "  - Split documents into manageable **text chunks** (e.g., by sections/headings).\n",
    "  - Compute **embeddings** for each chunk.\n",
    "  - Store chunks + metadata in a simple **index** (e.g., in memory or a local file).\n",
    "\n",
    "### 3.2 Tools / Functions\n",
    "\n",
    "We will implement Python functions (tools) that the LLM can call:\n",
    "\n",
    "- `retrieve_documents(query, project_filter=None)`  \n",
    "  → Returns top-k relevant text chunks + metadata.\n",
    "\n",
    "- `list_supported_projects()`  \n",
    "  → Returns the set of projects covered by the corpus.\n",
    "\n",
    "- `generate_tokenomics_image(prompt: str)`  \n",
    "  → Calls the external image generator and returns an image handle/URL.\n",
    "\n",
    "These tools will be exposed to the LLM via function-calling.\n",
    "\n",
    "### 3.3 LLM Pipeline Steps\n",
    "\n",
    "1. **Question Analysis (LLM Call 1)**\n",
    "   - Input: user question + list of known projects + tool descriptions.\n",
    "   - Output:\n",
    "     - Detected project(s),\n",
    "     - Question type/category,\n",
    "     - Decision whether an image might be useful.\n",
    "\n",
    "2. **Retrieval (Python + possibly LLM assistance)**\n",
    "   - Use `retrieve_documents(...)` with:\n",
    "     - The user question,\n",
    "     - Optional project filter from step 1.\n",
    "   - Produce a set of relevant chunks.\n",
    "\n",
    "3. **Answer Generation (LLM Call 2)**\n",
    "   - Input: user question + retrieved chunks.\n",
    "   - Output: answer containing:\n",
    "     - Structured explanation,\n",
    "     - References to specific chunks (e.g., “Source: Project X whitepaper, section 3.2”).\n",
    "\n",
    "4. **Answer Review (LLM Call 3, optional)**\n",
    "   - Input: question + retrieved chunks + draft answer.\n",
    "   - Output:\n",
    "     - Refined answer,\n",
    "     - Ensures citations and a short disclaimer.\n",
    "\n",
    "5. **Image Generation (Tool Call from LLM)**\n",
    "   - When applicable, the LLM:\n",
    "     - Constructs a **visual description** of the tokenomics/flow.\n",
    "     - Calls `generate_tokenomics_image(prompt=...)`.\n",
    "   - The notebook shows the resulting image after the call.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e15309",
   "metadata": {},
   "source": [
    "## 4. Implementation Plan \n",
    "\n",
    "We plan to implement the project in the following phases:\n",
    "\n",
    "1. **Phase 0 – Environment Setup**\n",
    "   - Set up Python environment and required libraries in VS Code.\n",
    "   - Create the initial Jupyter notebook structure.\n",
    "\n",
    "2. **Phase 1 – Data Collection & Preprocessing**\n",
    "   - Select 3–5 crypto projects.\n",
    "   - Download and convert whitepapers/docs to text.\n",
    "   - Implement chunking and embedding computation.\n",
    "   - Build a simple index (e.g., list or vector store).\n",
    "\n",
    "3. **Phase 2 – Basic RAG Prototype**\n",
    "   - Implement `retrieve_documents(query, project_filter)`.\n",
    "   - Build a simple “RAG answer” function:\n",
    "     - Single LLM call with user question + top-k chunks.\n",
    "   - Test with a few example queries.\n",
    "\n",
    "4. **Phase 3 – Multi-step LLM Pipeline**\n",
    "   - Implement the **question analysis** step.\n",
    "   - Integrate it with retrieval and answer generation.\n",
    "   - Add the optional **reviewer** step.\n",
    "   - Ensure that answers include **citations**.\n",
    "\n",
    "5. **Phase 4 – Image Generation Tool**\n",
    "   - Implement `generate_tokenomics_image(prompt)`.\n",
    "   - Integrate it as a function/tool the LLM can call.\n",
    "   - Create prompts for typical tokenomics diagrams.\n",
    "   - Test with queries that request visualizations.\n",
    "\n",
    "6. **Phase 5 – Evaluation, Documentation & Presentation Prep**\n",
    "   - Create a small set of test questions.\n",
    "   - Compare:\n",
    "     - Plain LLM vs. RAG-enhanced pipeline (qualitative evaluation).\n",
    "   - Document:\n",
    "     - Architecture,\n",
    "     - Implementation details,\n",
    "     - Limitations (e.g., small corpus, no financial advice),\n",
    "     - Potential improvements.\n",
    "   - Prepare example interactions for the final presentation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b53465",
   "metadata": {},
   "source": [
    "## 5. Limitations\n",
    "\n",
    "- **Small, curated corpus:**  \n",
    "  We only support the selected projects; the assistant cannot answer arbitrarily about all crypto.\n",
    "- **No financial advice:**  \n",
    "  The system is meant for **educational explanations** of documentation, not investment recommendations.\n",
    "- **Approximate understanding:**  \n",
    "  LLMs might still misinterpret nuanced technical details; hence we:\n",
    "  - Ground answers in retrieved text,\n",
    "  - Add disclaimers,\n",
    "  - Emphasize that answers are based on the documents provided.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1357482c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c8abfcd",
   "metadata": {},
   "source": [
    "# Structure"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9c9d993",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "project_root/\n",
    "  data/\n",
    "    raw_pdfs/      # downloaded whitepapers\n",
    "    texts/         # extracted text\n",
    "  src/\n",
    "    corpus.py      # loading and cleaning docs -> Person 1\n",
    "    indexer.py     # chunking + embeddings + retrieval -> Person 2\n",
    "    pipeline.py    # LLM pipeline (analysis → answer → review) -> Person 3\n",
    "    fine_tuning.py # Optional: fine-tuning logic (train + load) -> Person 3\n",
    "    imaging.py     # image generation tool -> Person 4\n",
    "  main.ipynb       # final demo + explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6366432",
   "metadata": {},
   "source": [
    "# Project Overview & Work Split\n",
    "\n",
    "We build an **LLM-based assistant** that explains and compares crypto projects using their whitepapers/docs, with **RAG**, a **multi-step pipeline**, an **image-generation tool**, and an **optional fine-tuning step**.\n",
    "\n",
    "---\n",
    "\n",
    "### Team & Main Components\n",
    "\n",
    "- **Person 1 – Corpus & Preprocessing** (`src/corpus.py`, `notebook/main.ipynb`)\n",
    "- **Person 2 – Index & Retrieval (RAG core)** (`src/indexer.py`)\n",
    "- **Person 3 – LLM Pipeline (+ Optional Fine-Tuning)** (`src/pipeline.py`, `src/fine_tuning.py`)\n",
    "- **Person 4 – Image Tool & Notebook Integration** (`src/imaging.py`)\n",
    "\n",
    "---\n",
    "\n",
    "### End-to-End Flow (High-Level)\n",
    "\n",
    "User question  \n",
    "→ **1. Question analysis (LLM / optionally fine-tuned classifier)**  \n",
    "→ **2. Retrieve relevant document chunks (RAG)**  \n",
    "→ **3. Generate answer with citations (LLM)**  \n",
    "→ **4. Optional review/refinement (LLM)**  \n",
    "→ **5. Optional image generation tool call**  \n",
    "→ **Final response: text answer + sources + (optional) image**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50712cb",
   "metadata": {},
   "source": [
    "# Component Breakdown & Responsibilities\n",
    "\n",
    "### 1. Corpus & Preprocessing  \n",
    "**Owner:** Person 1  \n",
    "**File:** `src/corpus.py`\n",
    "\n",
    "**Goal:** Turn PDFs/docs for **5–10 crypto projects** into clean text documents with metadata.\n",
    "\n",
    "**Steps / Tasks:**\n",
    "- Select projects (e.g. Bitcoin, Ethereum, Uniswap, Chainlink, Aave, …)\n",
    "- Download **official whitepapers/docs** and save to `data/raw_pdfs/`\n",
    "- Implement functions to:\n",
    "  - Load PDFs / HTML and extract text\n",
    "  - Clean text (remove headers/footers, duplicated content, weird spacing)\n",
    "  - Attach metadata (for each document), e.g.:  \n",
    "    - `project_id` (e.g. `\"bitcoin\"`)  \n",
    "    - `doc_id` (e.g. `\"btc_whitepaper\"`)  \n",
    "    - `source_path`\n",
    "- Return a list of document dicts, e.g.:  \n",
    "  ```python\n",
    "  [\n",
    "      {\"project\": \"bitcoin\", \"doc_id\": \"btc_whitepaper\", \"text\": \"...\"},\n",
    "      {\"project\": \"ethereum\", \"doc_id\": \"eth_whitepaper\", \"text\": \"...\"},\n",
    "      ...\n",
    "  ]\n",
    "\n",
    "**Deliverables:**\n",
    "- Function `load_corpus()` returning a list of documents with metadata\n",
    "- (Optional but helpful) Saved cleaned text files in `data/texts/`\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Index & Retrieval (RAG core)  \n",
    "**Owner:** Person 2  \n",
    "**File:** `src/indexer.py`\n",
    "\n",
    "**Goal:** Build a searchable **vector index** over all document chunks.\n",
    "\n",
    "**Steps / Tasks:**\n",
    "- Take the documents from `corpus.py` (`load_corpus()`)\n",
    "- Implement **chunking**:\n",
    "  - Split each full document (`text`) into smaller chunks (e.g. 300–800 tokens or by headings)\n",
    "  - For each chunk, keep metadata: `project`, `doc_id`, `chunk_id`, and the chunk `text`\n",
    "- Compute **embeddings** for each chunk using an embedding model\n",
    "- Store chunks + embeddings in a simple Python index object (e.g. a dict)\n",
    "- Implement:\n",
    "  - `build_index(documents: List[Dict[str, Any]]) -> Dict[str, Any]`\n",
    "    (build and return an index object or structure)\n",
    "  - `retrieve(question: str, index: Dict[str, Any], project_filter: Optional[List[str]] = None, k: int = 5) -> List[Dict[str, Any]]:`\n",
    "    (returns top-k chunks, each with metadata like `project`, `doc_id`, `chunk_id`, `text`, `score`)\n",
    "- Create small tests / demo code to:\n",
    "  - Run a sample query\n",
    "  - Print the top retrieved chunks\n",
    "\n",
    "**Deliverables:**\n",
    "- `build_index()` and `retrieve()` functions\n",
    "- Short description / docstring of how many projects & chunks the index contains\n",
    "\n",
    "---\n",
    "\n",
    "### 3. LLM Pipeline (Obligatory)\n",
    "**Owner:** Person 3  \n",
    "**File:** `src/pipeline.py`\n",
    "\n",
    "**Goal:** Orchestrate multiple LLM calls + retrieval into a coherent answer pipeline.\n",
    "\n",
    "**Core Steps / Tasks:**\n",
    "\n",
    "1. **Question Analysis**\n",
    "   - Implement `analyze_question(question: str, available_projects: list[str]) -> dict`\n",
    "   - Output example:\n",
    "     - `{\"projects\": [\"bitcoin\", \"ethereum\"], \"type\": \"comparison\", \"needs_image\": False}`\n",
    "   - Tasks:\n",
    "     - Detect which project(s) are mentioned or implied\n",
    "     - Classify question type (overview / tokenomics / risk / comparison / other)\n",
    "     - Decide if an image could be helpful (`needs_image` flag)\n",
    "\n",
    "2. **Retrieval**\n",
    "   - Implement `retrieve_for_question(question: str, analysis: dict, index) -> list[dict]`\n",
    "   - Tasks:\n",
    "     - Use `analysis[\"projects\"]` as a filter (if not empty)\n",
    "     - Call `indexer.retrieve(...)` with appropriate query and `k`\n",
    "     - Return a list of relevant chunks\n",
    "\n",
    "3. **Answer Generation**\n",
    "   - Implement `generate_answer(question: str, retrieved_chunks: list[dict]) -> dict`\n",
    "   - Output structure, for example:\n",
    "     - `\"answer_text\"`: final answer as string  \n",
    "     - `\"citations\"`: list of references (e.g. `[(project, doc_id, chunk_id), ...]`)  \n",
    "     - `\"raw_model_output\"`: (optional) raw LLM response\n",
    "   - Tasks:\n",
    "     - Provide the question + retrieved chunks to the LLM.\n",
    "     - Instruct the LLM to:\n",
    "       - Use only the given context for claims about projects\n",
    "       - Produce a clear structure (e.g. Summary / Details / Risks / Sources)\n",
    "       - Insert citations that map back to chunk metadata\n",
    "\n",
    "4. **Optional Answer Review**\n",
    "   - Implement `review_answer(question: str, retrieved_chunks: list[dict], draft_answer: dict) -> dict`\n",
    "   - Tasks:\n",
    "     - Second LLM call that:\n",
    "       - Checks grounding (is the answer consistent with the chunks?)\n",
    "       - Improves clarity / structure\n",
    "       - Adds or enforces disclaimer (e.g. “no financial advice”)\n",
    "   - Return a refined answer object (same structure as `generate_answer`)\n",
    "\n",
    "5. **Pipeline Wrapper**\n",
    "   - Implement `pipeline_answer(question: str, index, available_projects: list[str]) -> dict`\n",
    "   - Tasks:\n",
    "     - Call `analyze_question(...)`\n",
    "     - Call `retrieve_for_question(...)`\n",
    "     - Call `generate_answer(...)`\n",
    "     - Optionally call `review_answer(...)`\n",
    "     - Return a result dict used directly by the notebook, including:\n",
    "       - Final answer text\n",
    "       - Citations\n",
    "       - Any flags / metadata (e.g. whether an image is suggested)\n",
    "\n",
    "**Deliverables:**\n",
    "- Working `pipeline_answer(...)` function\n",
    "- Docstrings explaining each step and how they connect\n",
    "\n",
    "---\n",
    "\n",
    "### 3b. Optional Fine-Tuning (if time allows)  \n",
    "**Owner:** Person 3  \n",
    "**File:** `src/fine_tuning.py` (plus small changes in `src/pipeline.py`)\n",
    "\n",
    "**Goal (Option A):** Fine-tune a **question classifier** to make Step 1 (question analysis) more consistent and robust.\n",
    "\n",
    "**Steps / Tasks:**\n",
    "\n",
    "1. **Create Training Data (Synthetic or Semi-Manual)**\n",
    "   - Decide on labels:\n",
    "     - `project` label: e.g. `\"bitcoin\"`, `\"ethereum\"`, `\"uniswap\"`, `\"multi\"`, `\"unknown\"`\n",
    "     - `type` label: `\"overview\"`, `\"tokenomics\"`, `\"risk\"`, `\"comparison\"`, `\"other\"`\n",
    "   - Build a small dataset of examples:\n",
    "     - User-style questions mapped to `(project_label, type_label)`\n",
    "   - Save dataset as JSONL/CSV for fine-tuning\n",
    "\n",
    "2. **Run Fine-Tuning**\n",
    "   - Implement `run_fine_tuning_job(training_data_path: str) -> str`\n",
    "     - Starts a fine-tuning job via API/client\n",
    "     - Returns a `model_id` of the fine-tuned classifier\n",
    "   - Document:\n",
    "     - Base model used\n",
    "     - Basic settings (epochs, etc.)\n",
    "     - Final `model_id`\n",
    "\n",
    "3. **Use Fine-Tuned Classifier in Pipeline**\n",
    "   - Implement `classify_question_with_finetuned_model(question: str, available_projects: list[str]) -> dict`\n",
    "     - Returns same structure as `analyze_question(...)`\n",
    "   - Update `pipeline.py`:\n",
    "     - Add a config flag or parameter to switch between:\n",
    "       - Base model `analyze_question(...)`, and\n",
    "       - Fine-tuned classifier `classify_question_with_finetuned_model(...)`\n",
    "   - In the notebook, show a small comparison:\n",
    "     - The same questions analyzed by base vs. fine-tuned classifier\n",
    "\n",
    "**Deliverables:**\n",
    "- `fine_tuning.py` with training + inference helpers\n",
    "- Example outputs and short discussion in the notebook:\n",
    "  - How fine-tuning was done\n",
    "  - Before/after examples of question analysis\n",
    "\n",
    "*(This part is optional and should not block the main pipeline.)*\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Image Generation Tool & Notebook Integration  \n",
    "**Owner:** Person 4  \n",
    "**Files:** `src/imaging.py`, `notebook/main.ipynb`\n",
    "\n",
    "**Goal:** Allow the LLM to call an **image-generation tool** to produce tokenomics/flow diagrams.\n",
    "\n",
    "**Steps / Tasks:**\n",
    "\n",
    "1. **Image Tool Implementation**\n",
    "   - Implement `generate_tokenomics_image(prompt: str) -> str`\n",
    "     - Calls an external image generation API\n",
    "     - Returns an **image URL or file path**\n",
    "   - Handle basic errors (API not available, etc.)\n",
    "   - Optionally save images in `data/images/`\n",
    "\n",
    "2. **Tool Integration with LLM**\n",
    "   - Define `generate_tokenomics_image` as an **LLM tool / function-call**\n",
    "   - Decide integration logic:\n",
    "     - If `analysis[\"needs_image\"] == True`, or\n",
    "     - If the user explicitly asks for a visualization (keywords like \"draw\", \"diagram\", \"visualize\")\n",
    "   - Let the LLM construct a visual prompt using:\n",
    "     - The user question\n",
    "     - Key information from retrieved chunks\n",
    "     - The structure of the final answer\n",
    "\n",
    "3. **Notebook & Demo Integration**\n",
    "   - In `notebook/main.ipynb`:\n",
    "     - Load corpus & index\n",
    "     - Define a simple interface to:\n",
    "       - Enter or select a question\n",
    "       - Call `pipeline_answer(...)`\n",
    "       - Optionally trigger image generation (automatically or via a flag)\n",
    "     - Display:\n",
    "       - Final answer text\n",
    "       - Citations (e.g. short listing of sources / chunk IDs)\n",
    "       - Generated image inline (if possible) or as a link\n",
    "\n",
    "**Deliverables:**\n",
    "- `imaging.py` with a working image tool function\n",
    "- A clear demo in the notebook showing:\n",
    "  - Question → pipeline → answer + sources + (optional) image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
